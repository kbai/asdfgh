%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.

\documentclass[12pt]{article}
\usepackage[margin=0.75in]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{amsmath}
\usepackage{multirow}


\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose}


\makeatletter
\@ifundefined{date}{}{\date{}}
\makeatother

\begin{document}



\section{Problem 1}


\subsection*{a}

The model parameter vector is $\mathbf{m}=[P,d]^{T}$;


\subsection*{b}

$[\frac{\partial u}{\partial P},\frac{\partial u}{\partial d}]=[\frac{d}{(x^{2}+y^{2}+d^{2})^{3/2}},\frac{P}{(x^{2}+y^{2}+d^{2})^{3/2}}-\frac{3Pd^{2}}{(x^{2}+y^{2}+d^{2})^{5/2}}]$;


\subsection*{c}

$\begin{bmatrix}\frac{\partial^{2}u}{\partial P^{2}} & \frac{\partial^{2}u}{\partial P\partial d}\\
\frac{\partial^{2}u}{\partial d\partial P} & \frac{\partial^{2}u}{\partial d^{2}}
\end{bmatrix}=\begin{bmatrix}0 & \frac{x^{2}+y^{2}-2d^{2}}{(x^{2}+y^{2}+d^{2})^{5/2}}\\
\frac{x^{2}+y^{2}-2d^{2}}{(x^{2}+y^{2}+d^{2})^{5/2}} & \frac{6Pd^{3}-9pdx^{2}-9pdy^{2}}{(x^{2}+y^{2}+d^{2})^{7/2}}
\end{bmatrix}$

Note that the $\frac{\partial^{2}u}{\partial P\partial d}=\frac{\partial^{2}u}{\partial d\partial P}$
given that the function $u(P,d)$ is smooth enough.


\subsection*{d}

No, since the two chambers may have different locations$(x,y,d)$
and different P.

The model parameter vector for two magma chambers should be $[P_{1},P_{2},d_{1},d_{2},x_{1},x_{2},y_{1},y_{2}]^{T}$

and the observed vertical displacement should be:

$u=\frac{P_{1}d_{1}}{((x-x_{1})^{2}+(y-y_{1})^{2}+d_{1}^{2})^{3/2}}+\frac{P_{2}d_{2}}{((x-x_{2})^{2}+(y-y_{2})^{2}+d_{2}^{2})^{3/2}}$

If there are two magma chambers , it is not easy to tell their locations
$(x,y)$ in the first place. So they need to be inverted from observations.
Also we need to have two different depth (d) parameters and P(pressure
related parameters) to describe the two magma chamber.


\subsection*{e}

8


\subsection*{f}

No, because there are 8 degrees of freedom in the model space while
only 6 degrees of freedom in the data space.

To map from a 8 dimensional model space to 6 dimensional data space
means that there will be different models being mapped to the same
point in data space.

This causes the inversion to have multiple solutions and trade off
between different parameters.


\section{Problem 2}


\subsection*{a}

Let the matrix $\boldsymbol{A}=\left[\boldsymbol{x}_{1},\boldsymbol{x}_{2},\boldsymbol{x}_{3},\boldsymbol{x}_{4}\right]$,
then \\
 
\begin{eqnarray*}
\boldsymbol{A}= & \begin{bmatrix}1 & 0 & 0 & 1\\
0 & 1 & -1 & 1\\
0 & 1 & 1 & 1
\end{bmatrix}\\
\boldsymbol{A}^{T}\boldsymbol{A}= & \begin{bmatrix}1 & 0 & 0 & 1\\
0 & 2 & 0 & 2\\
0 & 0 & 2 & 0\\
1 & 2 & 0 & 3
\end{bmatrix}\\
= & \begin{bmatrix}\boldsymbol{x}_{1}^{T}\boldsymbol{x}_{1} & \boldsymbol{x}_{1}^{T}\boldsymbol{x}_{2} & \boldsymbol{x}_{1}^{T}\boldsymbol{x}_{3} & \boldsymbol{x}_{1}^{T}\boldsymbol{x}_{4}\\
 & \boldsymbol{x}_{2}^{T}\boldsymbol{x}_{2} & \boldsymbol{x}_{2}^{T}\boldsymbol{x}_{3} & \boldsymbol{x}_{2}^{T}\boldsymbol{x}_{4}\\
 & \multirow{2}{*}{\makebox[0pt]{\text{sym.}}} & \boldsymbol{x}_{3}^{T}\boldsymbol{x}_{3} & \boldsymbol{x}_{3}^{T}\boldsymbol{x}_{4}\\
 &  &  & \boldsymbol{x}_{4}^{T}\boldsymbol{x}_{4}
\end{bmatrix}
\end{eqnarray*}
Therefore, we see that $\boldsymbol{x}_{1}$, $\boldsymbol{x}_{2}$,
$\boldsymbol{x}_{3}$ are orthogonal to each other; and $\boldsymbol{x}_{3}$
and $\boldsymbol{x}_{4}$ are orthogonal.\\
 \\
 


\subsection*{b}

There are two independent vectors in $\left\{ \boldsymbol{x}_{1},\boldsymbol{x}_{2},\boldsymbol{x}_{4}\right\} $,
choosing any two from them, along with $\boldsymbol{x}_{3}$, which
is independent with all of them, form a basis. All the bases are: 

\begin{center}
$\left\{ \boldsymbol{x}_{1},\boldsymbol{x}_{2},\boldsymbol{x}_{3}\right\} $,
$\left\{ \boldsymbol{x}_{1},\boldsymbol{x}_{4},\boldsymbol{x}_{3}\right\} $,
$\left\{ \boldsymbol{x}_{2},\boldsymbol{x}_{4},\boldsymbol{x}_{3}\right\} $ 
\par\end{center}


\subsection*{c}

By definition\\
\begin{eqnarray*}
 \boldsymbol{AX}= & \boldsymbol{XD}
 \end{eqnarray*}
 where\\
\begin{eqnarray*}
\boldsymbol{X}= & \left[\boldsymbol{x}_{1}\quad\boldsymbol{x}_{2}\quad\boldsymbol{x}_{3}\right]\\
= & \begin{bmatrix}1 & 0 & 0\\
0 & 1 & -1\\
0 & 1 & 1
\end{bmatrix}\\
\boldsymbol{D}= & \begin{bmatrix}\lambda_{1} & 0 & 0\\
0 & \lambda_{2} & 0\\
0 & 0 & \lambda_{3}
\end{bmatrix}\\
= & \begin{bmatrix}2 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 4
\end{bmatrix}\\
\end{eqnarray*}
\\
Then\\
\begin{center}
$\boldsymbol{A}=\boldsymbol{XDX^{-1}}$ 
\par\end{center}

Since 
\begin{eqnarray*}
det(\boldsymbol{X})= & 2\\
\\
\boldsymbol{X}^{-1}= & 1/2\begin{bmatrix}2 & 0 & 0\\
0 & 1 & -1\\
0 & 1 & -1
\end{bmatrix}^{T}\\
= & \begin{bmatrix}1 & 0 & 0\\
0 & 0.5 & 0.5\\
0 & -0.5 & 0.5
\end{bmatrix}
\end{eqnarray*}


Then 
\begin{eqnarray*}
\boldsymbol{A}= & \begin{bmatrix}1 & 0 & 0\\
0 & 1 & -1\\
0 & 1 & 1
\end{bmatrix}\begin{bmatrix}2 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 4
\end{bmatrix}\begin{bmatrix}1 & 0 & 0\\
0 & 0.5 & 0.5\\
0 & -0.5 & 0.5
\end{bmatrix}= & \begin{bmatrix}2 & 0 & 0\\
0 & 4.5 & 0.5\\
0 & 0.5 & 4.5
\end{bmatrix}
\end{eqnarray*}
\\



\subsection*{d}

The vector is stretched by $\lambda_{1}$, $\lambda_{2}$, $\lambda_{3}$
along the corresponding eigenvector directions $\boldsymbol{x}_{1}$, $\boldsymbol{x}_{2}$, $\boldsymbol{x}_{3}$. \\
\\
Check it with $\boldsymbol{x}=[1\quad2\quad3]^{T}$.\\
\\
Let $\boldsymbol{v}_{1}=\boldsymbol{X^{-1}x}=[1\quad2.5\quad0.5]^{T}$\\
  \begin{eqnarray*}
  \boldsymbol{x} =& \boldsymbol{X X^{-1} x} \\=& \boldsymbol{X v_{1}} \\=& [\boldsymbol{x}_{1} \quad \boldsymbol{x}_{2}  \quad \boldsymbol{x}_{3}]
  \begin{bmatrix}
  1\\
  2.5\\
  0.5
  \end{bmatrix} \\=& 1\cdot \boldsymbol{x}_{1} + 2.5\cdot \boldsymbol{x}_{2} + 0.5\cdot \boldsymbol{x}_{3}
  \end{eqnarray*}
We see that $\boldsymbol{v}_{1}$ contains the coordinates of $\boldsymbol{x}$ in the basis $\left\{ \boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3} \right\}$.
\\
Then
\begin{eqnarray*}
\boldsymbol{Ax} =& \boldsymbol{XDX^{-1}x} \\
                                 =& \boldsymbol{XD}\boldsymbol{v}_{1}\\
                                 =& [\lambda_{1} \boldsymbol{x}_{1} \quad \lambda_{2} \boldsymbol{x}_{2}\quad  \lambda_{3} \boldsymbol{x}_{3}]  \boldsymbol{v}_{1}\\ 
                                 =& \lambda_{1} \cdot 1\cdot \boldsymbol{x}_{1} + \lambda_{2} \cdot 2.5\cdot \boldsymbol{x}_{2} + \lambda_{3} \cdot 0.5\cdot \boldsymbol{x}_{3}  \\                                                             
\end{eqnarray*}
which we see is a stretch of $\boldsymbol{x}$ by $\lambda_{i}$ in $\boldsymbol{x}_{i}$.\\
This transformation gives \\$  2 \cdot [1\quad 0 \quad 0]^{T} + 12.5 \cdot [0\quad 1 \quad 1]^{T} + 2 \cdot [0\quad -1 \quad1]^{T} \\
=  [2\quad10.5\quad14.5]^{T} $, which is equal to the direct multiplication of $\boldsymbol{Ax}$.
\section{Problem 3}


\subsection*{a}

This implies $\mathbf{v^{T}x}=0$ since $\mathbf{v}\in V$ means that
$\mathbf{v}$ is orthogonal to $\mathbf{x}$.


\subsection*{b}

This is explaining that the set $V$ is a subspace of $\mathbf{R}^{n}$

$(a_{1}\mathbf{v}_{\mathbf{1}}+a_{2}\mathbf{\mathbf{v}}_{2})^{T}\mathbf{x}=a_{1}\mathbf{v_{1}^{T}x}+a_{2}\mathbf{v_{2}^{T}x}=a_{1}(0)+a_{2}(0)=0$

This implies that $(a_{1}\mathbf{\mathbf{v}}_{1}+a_{2}\mathrm{\mathbf{v}}_{2})$
is also orthogonal to $\mathbf{x}$.

so $(a_{1}\mathbf{\mathbf{v}}_{1}+a_{2}\mathbf{v_{2}})\in V$


\subsection*{c}

Since $\mathbf{x}$ is an eigenvector of $\mathbf{A}$, we assume
$\mathbf{\boldsymbol{Ax}}=k\mathbf{x}$ where $\mathbf{k}\in\mathbf{R}$

$(\mathbf{Av})^{T}\mathbf{x}=\mathbf{v^{T}A^{T}x}=\mathbf{v^{T}Ax}=\mathbf{v^{T}(Ax)}=\mathbf{v^{T}}k\mathbf{x}=k\mathbf{v^{T}x}=0$

So $\mathbf{Av}$ is also orthogonal to $\mathbf{x}$ $\forall\mathbf{v}\in V$


\subsection*{d}

Given condition: $\forall\mathbf{v}\in V(V\neq\{0\}),$if $\mathbf{\boldsymbol{Av}}\in V$
then one can find one eigenvector of $\mathbf{A}$ in subspace $V$.

$\mathbf{A}$ is a $n\times n$ matrix.

First we can find one eigenvector $\mathbf{x}_{1}$ in $\mathbf{R}^{n}$and
we get the subspace $V_{1}\subseteq R^{n}$ that is orthogonal to
$\mathbf{x}_{1}.$

Then we can find another eigenvector $\mathbf{x_{2}}$in $V_{1}$
and get the subspace $V_{2}\subseteq V_{1}$ that is orthogonal to
$\mathbf{x}_{1}$and $\mathbf{x}_{2}$.

.........

Finally we find $\mathbf{x_{n}}$ in $V_{n-1}$ and get the subspace
$V_{n}\subseteq V_{n-1}$ that is orthogonal to $\mathbf{x_{1},x_{2},...x_{n}}$.

To this step, we have found all the eigenvectors that is orthogonal
to each other.

This proof is enough for our purpose. We also attached the following
proof that is more strict in a mathematical sense.

1) one can find one eigen vector $\mathbf{x}$ in $\mathbf{R^{n}}$.
Since $\mathbf{R}^{n}$satisfy Given condition. This is trivial.

2) if a subspace $V$ satisfy Given condition, and we have an eigen
vector $\mathbf{x}\in V$. Then we prove that the set $U=\{\mathbf{u}|\mathbf{u}^{T}\mathbf{x}=0,\mathbf{u}\in V\}$
also satisfy Given condition.

It is shown in (b) that $U$ forms subspace .

$\forall\mathbf{u}\in U,$$\mathbf{Au}\in V$ since $U\subseteq V$

we can write $\mathbf{Au}=c\mathbf{x}+\mathbf{w},\mathbf{w}\in U,c\in R$,
multiply both sides by $\mathbf{x}^{T}$:

$\boldsymbol{\mathbf{x}^{T}\mathbf{Au}}=c\boldsymbol{\mathbf{x^{T}x}}+\boldsymbol{\mathbf{x^{T}w}}=c\mathbf{\boldsymbol{x^{T}x}}$

note that on the left hand side:$\mathbf{\boldsymbol{x^{T}Au}}=\mathbf{\boldsymbol{u^{T}A^{T}x}}=\boldsymbol{\mathbf{u^{T}Ax}}=k\mathbf{\boldsymbol{u^{T}x}}=0$

So $c\boldsymbol{\mathbf{x^{T}x}}=c|\boldsymbol{\mathbf{x}}|^{2}=0,$which
implies that $c=0,$

So $\mathbf{\boldsymbol{Au}}=\mathbf{\boldsymbol{w}}\in U,\forall\mathbf{\boldsymbol{u}}\in U$.
So $U$ satisfy Given condition.

3) That means we can always find one eigen vector while the remaining
subspace still satisfy Given condition until it shrinks to $\{\boldsymbol{0}\}.$
Since each time , the dimension of the space is decreased by 1, we
can find n orthogonal eigen vectors.


\section{Problem 4}
(See atttached Matlab code and figures)\\
Fig. 1 shows the function and the four roots.\\
Fig. 2 shows the root found from each initial guess $x_{0}$. The red dots mark the positions of (root, root). We see for $x_{0}$ close to a certain root, the method will find this root. For $x_{0}$ that is not so close to any of the roots, there is no guarantee that the method will find the root that is closest to the initial guess.\\
Fig. 3 shows that for all the initial guesses, the method converges within a certain number of iterations.\\
\\
There are four roots of this function. If the initial guess is close
enough to a given root, Newton's method will find this root that is
closest to the initial guess. \\
Otherwise, if the initial guess is far away, there is no guarantee
that Newton's method will find the closest root. The \char`\"{}jump\char`\"{}
in x is controlled by the gradient at current point. If the current point is close to an extreme point (and note that there are many extreme points of this function), the small gradient will cause a large jump to the neighbor of another root. \\
To find all the roots of the function, we need to try different initial guesses.

\end{document}
