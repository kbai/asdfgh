%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{article}
\usepackage[latin9]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

\makeatother

\usepackage{babel}
\begin{document}

\section*{Problem 1}


\subsection*{(a)}

The model parameter vector $\boldsymbol{m}=[x_{s},y_{s},z_{s},P]^{T}$.
The forward model is nonlinear, since the partial derivatives $\frac{\partial G}{\partial m_{i}}$
are not constant.


\subsection*{(b)}

For least square problem, we introduce the objective function:

\[
\phi=(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d})^{T}(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d})=\underset{}{\overset{n}{\underset{i=1}{\sum}}(\frac{Pz_{s}}{[(x_{s}-x_{i})^{2}+(y_{s}-y_{i})^{2}+z_{s}^{2}]^{3/2}}-d_{i})^{2}}
\]


define : 
\[
R_{i}=(x_{s}-x_{i})^{2}+(y_{s}-y_{i})^{2}+z_{s}^{2}
\]
 

\[
lx_{i}=x_{i}-x_{s}
\]
 

\[
ly_{i}=y_{i}-y_{s}
\]


\[
A_{i}=\frac{Pz_{s}}{[(x_{s}-x_{i})^{2}+(y_{s}-y_{i})^{2}+z_{s}^{2}]^{3/2}}-d_{i}
\]


We write the Jacobian matrix: 
\[
\boldsymbol{J}=\boldsymbol{\nabla}_{\boldsymbol{m}}G=\frac{\partial G_{i}}{\partial m_{j}}
\]
 
\[
\boldsymbol{J}=\begin{bmatrix}\frac{3Pz_{x}lx_{1}}{2R_{1}^{5/2}} & \frac{3Pz_{x}ly_{1}}{2R_{1}^{5/2}} & \frac{PR_{1}-3Pz_{s}^{2}}{2R_{1}^{5/2}} & \frac{z_{s}}{2R_{1}^{3/2}}\\
\frac{3Pz_{x}lx_{2}}{2R_{2}^{5/2}} & \frac{3Pz_{x}ly_{2}}{2R_{2}^{5/2}} & \frac{PR_{2}-3Pz_{s}^{2}}{2R_{2}^{5/2}} & \frac{z_{s}}{2R_{2}^{3/2}}\\
... & ... & ... & ...\\
\frac{3Pz_{x}lx_{n}}{2R_{n}^{5/2}} & \frac{3Pz_{x}ly_{n}}{2R_{n}^{5/2}} & \frac{PR_{n}-3Pz_{s}^{2}}{2R_{n}^{5/2}} & \frac{z_{s}}{2R_{n}^{3/2}}
\end{bmatrix}
\]


\[
\boldsymbol{\nabla_{m}}\phi=2(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d})^{T}\boldsymbol{J}=[\underset{i=1}{\overset{n}{\sum}(}\frac{Pz_{s}}{R_{i}^{3/2}}-d_{i})(\frac{3Pz_{x}lx_{i}}{R_{i}^{5/2}}),\underset{i=1}{\overset{n}{\sum}(}\frac{Pz_{s}}{R_{i}^{3/2}}-d_{i})(\frac{3Pz_{x}ly_{i}}{R_{i}^{5/2}}),\underset{i=1}{\overset{n}{\sum}}(\frac{Pz_{s}}{R_{i}^{3/2}}-d_{i})(\frac{PR_{i}-3Pz_{s}^{2}}{R_{i}^{5/2}}),\underset{i=1}{\overset{n}{\sum}}(\frac{Pz_{s}}{R_{i}^{3/2}}-d_{i})(\frac{z_{s}}{R_{i}^{3/2}})]^{T}
\]


\[
\boldsymbol{H}(\phi)=\boldsymbol{\nabla_{m}}(\boldsymbol{\nabla_{m}}\phi)=2\nabla(\boldsymbol{J}^{T}(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d}))=2J^{T}J+2(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d})^{T}\boldsymbol{\nabla J}
\]


\[
H_{approximate}=2\boldsymbol{J}^{T}\boldsymbol{J}
\]


\[
2(\boldsymbol{G}(\boldsymbol{m})-\boldsymbol{d})^{T}\boldsymbol{\nabla J}=\underset{i=1}{\overset{n}{\sum}}\frac{A_{i}}{R_{i}^{7/2}}\begin{bmatrix}15Pz_{s}lx_{i}^{2}-3Pz_{s}R_{i} & 15Pz_{s}lx_{i}ly_{i} & 3Plx_{i}R_{i}-15Pz_{s}^{2}lx_{i} & 3z_{s}lx_{i}R_{i}\\
 & 15Pz_{s}ly_{i}^{2}-3Pz_{s}R_{i} & 3Ply_{i}R_{i}-15Pz_{s}^{2}ly_{i} & 3z_{s}dy_{i}R_{i}\\
sym &  & 15Pz_{s}^{3}-9Pz_{s}R_{i} & R_{i}^{2}-9z_{s}^{2}R_{i}\\
 &  &  & 0
\end{bmatrix}
\]


(c)\include{compute_gradient_approx_hess}


\section*{Problem 2}


\section*{(a)}

Fig. 1 shows the $(x,y)$ points.


\section*{(b) }

$m_{1}$ is the intercept with the $y$ axis, we estimate from the
plot that it should be bounded by $[-150,0]$. \\
$m_{2}$ is the slope of the line, we estimate from the plot that
it should be bounded by $[1,10]$. \\
We can start with a larger model space, and do a coarse search with
$m1=[-150:0.1:0]$, and $m2=[1:0.01:10]$. If needed, we can shrink
our model space and do a finer search as a second step.


\subsection*{(c)-(d)}


\subsection*{(e)}

The optimum solutions are

L2 norm: (-75.5, 3.53)

L1 norm: (-81.9, 3.48)


\subsection*{(f)}

The optimum solution with least square is: (-75.4631, 3.5301)


\subsection*{(g)}

The standard result only gives the diagonal term in the model covariance
matrix, while the off-diagonal term can be shown in the error ellipse.
We see that the two model parameters are negatively correlated in
this problem.


\subsection*{(h)}

The L2 and L1 (grid search) method is more straighforward in showing
the error distribution, and the correlation between model parameters.
\\
The least-square solution is fast, and gives the exact solution that
minimizes L2 norm error. We can infer some information, such as the
correlations between the model parameters, from its output matrices.
However, it is not as straightforward. In this small size problem,
I would prefer the direct search method.\\
In this problem, we see that the data points are quite close to a
line, so we speculate that the errors are small. However, if the points
are more scattered as the errors are getting bigger, the L1 norm method
can be better, because it would be less likely to be affected by the
outliers. If there are several solutions that can minimize the error
equally well, we can see it in the error map produced by the direct
search method, and can choose one solution based on some priori information.
\end{document}
